[
  {
    "objectID": "Project-Model-Selection-Encelia.html",
    "href": "Project-Model-Selection-Encelia.html",
    "title": "Replace this with an interesting title",
    "section": "",
    "text": "Write a few sentences about Encelia californica, Encelia farinosa, and the Fullerton Arboretum. You may also discuss what you learned about the iris dataset.\nWhy should someone care about being able to distinguish between these two species?"
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#motivation-and-context",
    "href": "Project-Model-Selection-Encelia.html#motivation-and-context",
    "title": "Replace this with an interesting title",
    "section": "",
    "text": "Write a few sentences about Encelia californica, Encelia farinosa, and the Fullerton Arboretum. You may also discuss what you learned about the iris dataset.\nWhy should someone care about being able to distinguish between these two species?"
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#main-objective",
    "href": "Project-Model-Selection-Encelia.html#main-objective",
    "title": "Replace this with an interesting title",
    "section": "Main Objective",
    "text": "Main Objective\nIn your own words, describe the goal of this project."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#packages-used-in-this-analysis",
    "href": "Project-Model-Selection-Encelia.html#packages-used-in-this-analysis",
    "title": "Replace this with an interesting title",
    "section": "Packages Used In This Analysis",
    "text": "Packages Used In This Analysis\n\nlibrary(here)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(rsample)\nlibrary(purrr)\nlibrary(yardstick)\nlibrary(tidyr)\n\n\n\n\nPackage\nUse\n\n\n\n\nhere\nto easily load and save data\n\n\nreadr\nto import the CSV file data\n\n\ndplyr\nto massage and summarize data\n\n\nggplot2\nto create nice-looking and informative graphs\n\n\nrsample\nto split data into training and test sets\n\n\npurrr\nto run the cross-validation\n\n\nyardstick\nto evalute the accuracy of the models\n\n\ntidyr\nto “pivot” the predictions data frame so that each row represents 1 model"
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#design-and-data-collection",
    "href": "Project-Model-Selection-Encelia.html#design-and-data-collection",
    "title": "Replace this with an interesting title",
    "section": "Design and Data Collection",
    "text": "Design and Data Collection\nIn this section, describe how you collected the data. (If you weren’t in class that day, ask your classmates how the data was collected.)\nBriefly describe the limitations of your data collection method. What difficulties did you (or the class collectively) have collecting the data? What choices did you (or the class collectively) make during the data collection process that someone else might choose differently?"
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#training-test-split",
    "href": "Project-Model-Selection-Encelia.html#training-test-split",
    "title": "Replace this with an interesting title",
    "section": "Training-Test Split",
    "text": "Training-Test Split\nThere isn’t much data massaging we need to do here, because we were very deliberate about how we set up our data sheet and how we recorded the data on it. However, you may need to do things like import the data and change the type of some variables.\nAt the end of this section, you should write code to randomly split the Encelia data into a training and test set and explain why a training and test set are useful/necessary for this objective."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#exploratory-data-analysis",
    "href": "Project-Model-Selection-Encelia.html#exploratory-data-analysis",
    "title": "Replace this with an interesting title",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nExpand on the EDA you did in the Logistic Regression with Encelia activity. Explain what is going on in your graphs and summaries and how those insights relate to the models you will be building in the next section."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#modeling",
    "href": "Project-Model-Selection-Encelia.html#modeling",
    "title": "Replace this with an interesting title",
    "section": "Modeling",
    "text": "Modeling\nPropose several logistic regression models and use cross-validation to select a best model, following the steps in the “Cross-Validation (Part 3)” video and the “Model Selection for Logistic Regression” activity.\nExplain what your code is doing:\n\nWhat is logistic regression? Why are you doing it?\nWhy did you choose each model that you are considering?\nWhy are you using cross-validation? How does it work?\nWhich model are you selecting as the best model? Why?\n\nFit the selected “best” model on the training set and make predictions on the test set. Overall, how accurately is your model making predictions?"
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#insights",
    "href": "Project-Model-Selection-Encelia.html#insights",
    "title": "Replace this with an interesting title",
    "section": "Insights",
    "text": "Insights\nThink about how you might visualize the model’s predictions and the accuracy of those predictions. Then, create and describe one or more visualizations in line with your ideas.\nWhich flowers (if any) were incorrectly predicted? Why do you suspect that they were incorrectly predicted?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Project1.html",
    "href": "Project1.html",
    "title": "Project 1",
    "section": "",
    "text": "##This reads in the data as well as any packages that are necessary. \nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nhospital &lt;- read_csv(\"Data/hospital.csv\", na=c(\"Not Applicable\", \"Not Available\"))\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 443517 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): Facility ID, Facility Name, Address, City/Town, State, ZIP Code, C...\ndbl  (6): Patient Survey Star Rating, Patient Survey Star Rating Footnote, H...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#We removed any of the data that said \"Not Applicable\" or \"Not Available\" and replaced it with a N/A that would help for further data analysis. \n\n\n##This is to only keeping what I find relevant in my dataset\nhospital &lt;- hospital |&gt;\n  select(\n    `Facility ID`,\n    `Facility Name`, \n    `HCAHPS Answer Description`, \n    `Patient Survey Star Rating`, \n     `HCAHPS Linear Mean Value`,\n    `HCAHPS Answer Percent`,\n    `Survey Response Rate Percent`,\n    `Number of Completed Surveys`\n  )\n\n\n##clean the data & rename things\nhospital_clean &lt;- hospital |&gt;\n  rename(\n    answer = `HCAHPS Answer Description`,\n    star_rating = `Patient Survey Star Rating`,\n    mean_value = `HCAHPS Linear Mean Value`,\n    answer_percent = `HCAHPS Answer Percent`,\n    fac_id = `Facility ID`,\n    fac_name = `Facility Name`, \n    response_rate = `Survey Response Rate Percent`,\n    surveys = `Number of Completed Surveys`\n  )\n\nMy goal for this project is to see what factors are the heaviest influences on the overall and recommended scores that are reported.\nI want to group by rating system: star rating, linear mean value, and answer percent.\n\n#This groups the data by the rating systems: star, linear mean value, and answer percent. \nhospital.star &lt;- hospital_clean |&gt;\n  select(\n    fac_id,\n    fac_name, \n    answer, \n    star_rating, \n    response_rate,\n    surveys\n  )|&gt;\n  filter( \n    !is.na(star_rating)\n  )\n\nhospital.lmv &lt;- hospital_clean |&gt;\n  select(\n    fac_id,\n    fac_name, \n    answer, \n    mean_value, \n    response_rate, \n    surveys\n  )|&gt;\n  filter( \n    !is.na(mean_value)\n  )\n\nhospital.answer &lt;- hospital_clean |&gt;\n   select(\n    fac_id,\n    fac_name, \n    answer, \n    answer_percent, \n    response_rate,\n    surveys\n  )|&gt;\n  filter( \n    !is.na(answer_percent))\n\n\n##label: This is trying to pivot wider so that we can visualize the data in a different way. \nhospital.lmv &lt;- pivot_wider(hospital.lmv, names_from = answer, values_from = mean_value)\nhospital.answer &lt;- pivot_wider(hospital.answer, names_from = answer, values_from = answer_percent)\nhospital.star &lt;- pivot_wider(hospital.star, names_from = answer, values_from = star_rating)\n\nIn looking at the data, the more specific data set would be the linear mean score, hence, I will be continuing to observe with just that data set.\n\n#This renames the variables to shorten them for further coding ease.\nhospital.lmv &lt;- hospital.lmv |&gt;\n  rename(\n      nurse_com = `Nurse communication - linear mean score`,\n      doc_com = `Doctor communication - linear mean score`,\n      staff_response = `Staff responsiveness - linear mean score`,\n      med_coms = `Communication about medicines - linear mean score`, \n      discharge_info = `Discharge information - linear mean score`,\n      care_transition = `Care transition - linear mean score`,\n      clean = `Cleanliness - linear mean score`,\n      quiet = `Quietness - linear mean score`,\n      overall = `Overall hospital rating - linear mean score`,\n      rec = `Recommend hospital - linear mean score`\n  )\n\nhospital.star &lt;- hospital.star |&gt;\n  rename(\n      nurse_com = `Nurse communication - star rating`,\n      doc_com = `Doctor communication - star rating`,\n      staff_response = `Staff responsiveness - star rating`,\n      med_coms = `Communication about medicines - star rating`, \n      discharge_info = `Discharge information - star rating`,\n      care_transition = `Care transition - star rating`,\n      clean = `Cleanliness - star rating`,\n      quiet = `Quietness - star rating`,\n      overall = `Overall hospital rating - star rating`,\n      rec = `Recommend hospital - star rating`\n  )\n\n\n##This gets the average of the scores that could affect the overall or recommended score and creates a new column for that data point for each hospital. \nhospital.lmv &lt;- hospital.lmv |&gt;\n  mutate( \n    average_score = rowMeans(subset(hospital.lmv, select = c(nurse_com, doc_com, staff_response, med_coms, discharge_info, care_transition, clean, quiet)), na.rm = TRUE)\n  )\n\n\n##This finds the differences in the overall and recommendation scores versus the calculated average score. \nhospital.lmv &lt;- hospital.lmv |&gt;\n  mutate(\n    o.a = overall - average_score, \n    r.a = rec - average_score\n  )\n\nThe section above does not do too much for the initial goal of the project, but it is an interesting thing to see how they compare.\nNow, what I want to find is which of the variables has the most impact on the recommended and overall scores, and I will do this through a multiple linear regression model.\n\n#This is my linear regression model for the given overall score. \noverall.lm &lt;- lm(overall ~ nurse_com + doc_com +  staff_response + med_coms + discharge_info + care_transition + clean + quiet, data = hospital.lmv)\n\nsummary(overall.lm)\n\n\nCall:\nlm(formula = overall ~ nurse_com + doc_com + staff_response + \n    med_coms + discharge_info + care_transition + clean + quiet, \n    data = hospital.lmv)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7876  -0.8752   0.0497   0.9688   5.3160 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -17.420251   1.284799 -13.559  &lt; 2e-16 ***\nnurse_com         0.375618   0.026356  14.252  &lt; 2e-16 ***\ndoc_com           0.131559   0.018765   7.011 2.88e-12 ***\nstaff_response    0.030552   0.011555   2.644 0.008231 ** \nmed_coms         -0.041388   0.011441  -3.618 0.000302 ***\ndischarge_info    0.038515   0.011190   3.442 0.000585 ***\ncare_transition   0.514732   0.019467  26.442  &lt; 2e-16 ***\nclean             0.100597   0.007418  13.561  &lt; 2e-16 ***\nquiet             0.074207   0.006587  11.267  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.456 on 3188 degrees of freedom\nMultiple R-squared:  0.8596,    Adjusted R-squared:  0.8592 \nF-statistic:  2440 on 8 and 3188 DF,  p-value: &lt; 2.2e-16\n\nplot(overall.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#This is my linear regression model for the given reccommended score. \n\nrec.lm &lt;- lm(rec ~ nurse_com + doc_com + staff_response + med_coms + discharge_info + care_transition + clean + quiet, data = hospital.lmv)\n\nsummary(rec.lm)\n\n\nCall:\nlm(formula = rec ~ nurse_com + doc_com + staff_response + med_coms + \n    discharge_info + care_transition + clean + quiet, data = hospital.lmv)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.6046 -1.5158  0.1297  1.6339  6.7448 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -49.95953    2.07335 -24.096  &lt; 2e-16 ***\nnurse_com         0.51411    0.04253  12.088  &lt; 2e-16 ***\ndoc_com           0.15841    0.03028   5.231 1.79e-07 ***\nstaff_response   -0.05011    0.01865  -2.688  0.00724 ** \nmed_coms         -0.12499    0.01846  -6.770 1.53e-11 ***\ndischarge_info    0.02572    0.01806   1.424  0.15441    \ncare_transition   0.95662    0.03141  30.451  &lt; 2e-16 ***\nclean             0.08131    0.01197   6.793 1.31e-11 ***\nquiet             0.03342    0.01063   3.144  0.00168 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.349 on 3188 degrees of freedom\nMultiple R-squared:  0.7812,    Adjusted R-squared:  0.7807 \nF-statistic:  1423 on 8 and 3188 DF,  p-value: &lt; 2.2e-16\n\nplot(rec.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn reading the summaries and looking at the residual plots, I noticed a few things:\nFirstly, the linear model for the overall scores is generally a good fit. However, the recommendation score is not a great fit. There are four plots that we look at for determining if a linear model is a good fit. Those plots are Residuals vs. Fitted, Q-Q residuals, Scale-Location, and Residuals vs. Leverage. To make things short, we want certain shapes for each of these graphs to determine if the model is a good fit. For the Residuals vs. Fitted, Scale-Location, and Residuals vs. Leverage graphs, we want them generally to be scattered about a horizontal line. The Q-Q residuals is different in the sense that we want a diagonal line. For the Residuals vs. Leverage graph, we want our data to form a sort of cone shape that is limited by Cook’s distance. Data that is outside that distance tends to skew the model. Notice how neither of the models really go past that line.\nSecondly, there are a few data points that stick out and are labeled on the graphs. I did go back through the data to observe those, and they did have higher scores than anticipated, but I chalked it up to great experiences at the hospital.\nThirdly, we look at the r-squared values for each of the models in their summaries. Typically, the larger the value, the more precise the predictor variables are able to predict the overall or recommended score. With this knowledge, the overall linear model is a better model than the recommended model (0.8596 vs. 0.7812).\nLastly, I looked through the values for the predictive equation. The equations are as follows:\n\noverall = -17.42 + 0.38(nurse_com) + 0.13(doc_com) + 0.03(staff_response) - 0.04(med_coms) + 0.04 (discharge_info) + 0.51(care_transition) + 0.10(clean) + 0.07(quiet)\nrec = -49.96 + 0.51(nurse_com) + 0.16(doc_com) - 0.05 (staff_response) - 0.12(med_coms) + 0.03 (discharge_info) + 0.96(care_transition) + 0.08(clean) + 0.03(quiet)\n\nI was curious about what variable impacts the scores the most. With these equations, there is a heavy influence from the care transition for both models.\nLimitations:\nIf I were to receive this data to help me choose a hospital to go to, I would rely more on the overall scores to help me make my decision over the recommended score. However, this data should not be the only research done for these hospitals. These models do not always account for every different experience that a person would get, and some may be skewed positive or skewed negative experiences. The response rates for each of the hospitals varies incredibly, which can also be a factor in what data they receive. In looking at the data collection process, it seems that most of the surveys were of a voluntary response, indicating that mostly those who are passionate about their experience will respond. This may not be truly random data and should be taken with a grain of salt."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]