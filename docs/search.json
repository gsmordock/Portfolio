[
  {
    "objectID": "Project-Model-Selection-Encelia.html",
    "href": "Project-Model-Selection-Encelia.html",
    "title": "Encelia Data",
    "section": "",
    "text": "Encelia californica and Encelia farinosa are two bright yellow flowers that can be found in the Fullerton Arboretum. They can be used to restore deserts and native gardens and help with erosion control. It is important that people understand the differences between the two species if they are trying to restore areas and need the correct plant to do so. When restoring areas, it is important that the plant can flourish properly in the environment it is placed in. One of the biggest differences are when they flower. The Encelia farinosa flowers from February to May then August to September while Encelia californica flowers from February to October. Another difference is the elevations and locations where they thrive. Encelia farinosa can go up to 1000 meters above sea level and thrives in more inland desert regions, while the Encelia californica can go up to 600 meters above sea level and is found in more coastal regions.\nThe Fullerton Arboretum focuses on conserving and displaying the beauty of plants in a local community area. The Arboretum works with California State University, Fullerton to provide students with the ability to learn, research and appreciate these plants and horticulture."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#motivation-and-context",
    "href": "Project-Model-Selection-Encelia.html#motivation-and-context",
    "title": "Encelia Data",
    "section": "",
    "text": "Encelia californica and Encelia farinosa are two bright yellow flowers that can be found in the Fullerton Arboretum. They can be used to restore deserts and native gardens and help with erosion control. It is important that people understand the differences between the two species if they are trying to restore areas and need the correct plant to do so. When restoring areas, it is important that the plant can flourish properly in the environment it is placed in. One of the biggest differences are when they flower. The Encelia farinosa flowers from February to May then August to September while Encelia californica flowers from February to October. Another difference is the elevations and locations where they thrive. Encelia farinosa can go up to 1000 meters above sea level and thrives in more inland desert regions, while the Encelia californica can go up to 600 meters above sea level and is found in more coastal regions.\nThe Fullerton Arboretum focuses on conserving and displaying the beauty of plants in a local community area. The Arboretum works with California State University, Fullerton to provide students with the ability to learn, research and appreciate these plants and horticulture."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#main-objective",
    "href": "Project-Model-Selection-Encelia.html#main-objective",
    "title": "Encelia Data",
    "section": "Main Objective",
    "text": "Main Objective\nWith this project, we are using a few main differences of these plants to build a model that can distinguish which plant is which based on a few basic measurements."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#packages-used-in-this-analysis",
    "href": "Project-Model-Selection-Encelia.html#packages-used-in-this-analysis",
    "title": "Encelia Data",
    "section": "Packages Used In This Analysis",
    "text": "Packages Used In This Analysis\n\nlibrary(here)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(rsample)\nlibrary(purrr)\nlibrary(yardstick)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(broom)\n\n\n\n\nPackage\nUse\n\n\n\n\nhere\nto easily load and save data\n\n\nreadr\nto import the CSV file data\n\n\ndplyr\nto massage and summarize data\n\n\nggplot2\nto create nice-looking and informative graphs\n\n\nrsample\nto split data into training and test sets\n\n\npurrr\nto run the cross-validation\n\n\nyardstick\nto evalute the accuracy of the models\n\n\ntidyr\nto “pivot” the predictions data frame so that each row represents 1 model"
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#design-and-data-collection",
    "href": "Project-Model-Selection-Encelia.html#design-and-data-collection",
    "title": "Encelia Data",
    "section": "Design and Data Collection",
    "text": "Design and Data Collection\nAs a class, we went to the Fullerton Arboretum and identified the two flowers that we wanted to collect the data on. We split into pairs and collected four pieces of data on the two species: the number of rays on each flower, the diameter of the center disk, the diameter of the entire flower, and the stem length. Everything was measured in centimeters. We used a simple measuring ruler and recorded our data on a sheet of paper. The number of rays was counted individually. The diameter of the disk was measured through the center from end to end. The diameter of the entire flower was measured similarly to the disk but extended to include the rays. The stem length was measured from the bottom of the flower part of the plant to the first area that it branched off closest to the flower. Each pair measured 10 flowers of each species, so in total we got 50 of each species.\n\nA few limitations of the data collection was limited access. There were many flowers that could not be accessed due to the desire to respect the grounds and not create new and unnatural paths. A lot of the flowers were also close to more dangerous plants like cacti or in areas that were fenced off, so that limited the randomness of the collection. My partner and I decided to stay consistent with who was collecting the data. Since we did this, we tried to limit the variation in measuring judgement that may have occurred had we not stayed consistent, like deciding where the center of a flower was. Additionally, the measurements that we did were not always as accurate as we wanted them. The rulers only went so far in the parts to a whole so judgement in rounding could be different based on who did the measuring."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#training-test-split",
    "href": "Project-Model-Selection-Encelia.html#training-test-split",
    "title": "Encelia Data",
    "section": "Training-Test Split",
    "text": "Training-Test Split\nI’m going to read in the data and display it below.\n\nlibrary(readr)\nencelia &lt;- read_csv(\"Data/Encelia Classification Data Collection - Sheet1.csv\")\n\nRows: 100 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Species\ndbl (4): number_rays, disk_diameter, ray_diameter, stem_length\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nencelia &lt;- encelia|&gt;\n  mutate(\n    Species = Species |&gt;\n      as.factor()\n  )\n\ncontrasts(encelia$Species)\n\n  F\nC 0\nF 1\n\n\nThe data consists of numeric values which are the measurements of each variable in centimeters or whole numbers for the ray count, so we do not need to change anything.\n\nset.seed(3)\nencelia_split &lt;- initial_split(\n  encelia,\n  strata = Species,\n  prop = 0.80\n)\n\nencelia_train &lt;- training(encelia_split)\nencelia_test &lt;- testing(encelia_split)\n\nencelia_train$Species &lt;- as.factor(encelia_train$Species)\n\n#This creates the folds for later.\nencelia_cv &lt;- encelia_train |&gt;\n  vfold_cv(\n    v=4, #4 fold cross-validation\n    repeats = 1 #gives one unique prediction per observation in the training set\n  )\n\nA training set trains the model to be able to predict the outcome, and the test set will be put into the model to see how well it actually does predict. It will be useful for this objective because we want to build a model that can predict what species a flower is given the measurements. Additionally, I created the folds in the set that we can use to get the predictions."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#exploratory-data-analysis",
    "href": "Project-Model-Selection-Encelia.html#exploratory-data-analysis",
    "title": "Encelia Data",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nDuring the Logistic Regression with Encelia activity, we created plots that compared each of the variables to the species that it was. We knew that there were some differences to the flowers, and we were able to pinpoint the main identifiers that were different by comparing the two species. The graphs will help to identify which variables we should consider choosing for our model.\n\n#disk diameter vs. species\nggplot(\n  data = encelia_train,\n  mapping = aes(x = disk_diameter, y=Species)\n  ) + \n  geom_jitter(height = 0.1)\n\n\n\n\n\n\n\n\nThe disk diameter is fairly similar for both flowers so it might not be as useful when creating our model. The model might get confused since there is not a significant difference.\n\n#number of rays vs. species\nggplot(\n  data = encelia_train,\n  mapping = aes(x = number_rays, y=Species)\n  ) + \n  geom_jitter(height = 0.1)\n\n\n\n\n\n\n\n\nWe can see that the Encelia californica tended to have a higher number of rays than the Encelia farinosa, which is a known identifier for these species.\n\n#ray diameter vs. species\nggplot(\n  data = encelia_train,\n  mapping = aes(x = ray_diameter, y=Species)\n  ) + \n  geom_jitter(height = 0.1)\n\n\n\n\n\n\n\n\nThe ray diameter seems very similar for both flowers, but the average for the Encelia californica was higher overall.\n\n#stem length vs. species\nggplot(\n  data = encelia_train,\n  mapping = aes(x = stem_length, y=Species)\n  ) + \n  geom_jitter(height = 0.1)\n\nWarning: Removed 16 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nStem length for the Encelia californica had a wider range than the Encelia farinosa. With this knowledge, we could see that most of the Encelia farinosa had overall smaller stem lengths which could help us identify for our model.\nAfter looking at these plots, we can see which variables might be the most important when helping our model predict the species. Before looking at the modeling, it seems like the stem length and ray diameter might be the biggest factors in predicting which species a flower is."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#modeling",
    "href": "Project-Model-Selection-Encelia.html#modeling",
    "title": "Encelia Data",
    "section": "Modeling",
    "text": "Modeling\nLogistic regression is a machine model that attempts to distinguish between categories and is used for prediction. The goal of our project is to be able to distinguish which flower is which so this model will help us do that.\nCross-validation is important to see how well the model works to predict what we want. It involves separating the data into training and testing sets. The training set trains the model using that data, and the testing set tests if the model really works how it should.\n\nencelia_prediction &lt;- function(split) {\n\n#Step 1: create the training and validation sets \ntrain &lt;- analysis(split)\nvalid &lt;- assessment(split)\n\n#Step 2: Fit all the models on the training set\nencelia_glm1 &lt;- glm(Species ~ number_rays + disk_diameter + ray_diameter + stem_length, data = train, family = \"binomial\")\nencelia_glm2 &lt;- glm(Species ~ number_rays + ray_diameter + stem_length, data = train, family = \"binomial\")\nencelia_glm3 &lt;- glm(Species ~ disk_diameter + ray_diameter + stem_length, data = train, family = \"binomial\")\nencelia_glm4 &lt;- glm(Species ~ number_rays + disk_diameter + stem_length, data = train, family = \"binomial\")\nencelia_glm5 &lt;- glm(Species ~ number_rays + disk_diameter + ray_diameter, data = train, family = \"binomial\")\nencelia_glm6 &lt;- glm(Species ~ number_rays + stem_length, data = train, family = \"binomial\")\nencelia_null &lt;- glm(Species ~1, data = train, family = \"binomial\")\n\n#Step 3: Make all the predictions on the validation set\nvalid_predictions &lt;- valid |&gt;\n  mutate(\n    pred_all = predict(encelia_glm1, newdata = valid, type = \"response\"), \n    pred_no_disk = predict(encelia_glm2, newdata = valid, type = \"response\"), \n    pred_no_rays = predict(encelia_glm3, newdata = valid, type = \"response\"),\n    pred_no_diameter = predict(encelia_glm4, newdata = valid, type = \"response\"),\n    pred_no_stem = predict(encelia_glm5, newdata = valid, type = \"response\"),\n    pred_2 = predict(encelia_glm6, newdata = valid, type = \"response\"),\n    pred_null = predict(encelia_null, newdata = valid, type = \"response\")\n  )\nreturn(valid_predictions)\n}\n\n\n#This runs the function on the splits.\nmapped_pred &lt;- map(\n  encelia_cv$splits, \n  encelia_prediction\n)\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nmapped_pred_df &lt;- mapped_pred |&gt;\n  bind_rows(\n    .id = \"fold\"\n  )\n\nmapped_pred_df |&gt;\n  select(\n    fold, Species, pred_all, pred_no_disk, pred_no_rays, pred_no_diameter, pred_no_stem, pred_2, pred_null\n  )\n\n# A tibble: 79 × 9\n   fold  Species  pred_all pred_no_disk pred_no_rays pred_no_diameter\n   &lt;chr&gt; &lt;fct&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;\n 1 1     C        1.55e- 3      9.79e-3     4.62e- 3         7.99e- 3\n 2 1     C        8.43e- 6      3.16e-2     2.97e- 4         1.29e- 3\n 3 1     C        1.46e- 2      2.99e-2     2.73e- 5         4.59e- 5\n 4 1     C        4.52e- 5      5.60e-4     2.63e- 5         5.21e- 4\n 5 1     C       NA            NA          NA               NA       \n 6 1     C       NA            NA          NA               NA       \n 7 1     C       NA            NA          NA               NA       \n 8 1     C        1.68e- 5      1.60e-5     6.56e- 4         5.55e- 2\n 9 1     C        2.22e-16      6.17e-9     2.45e-12         2.02e-11\n10 1     C        2.59e- 3      1.28e-2     7.08e- 1         4.76e- 1\n# ℹ 69 more rows\n# ℹ 3 more variables: pred_no_stem &lt;dbl&gt;, pred_2 &lt;dbl&gt;, pred_null &lt;dbl&gt;\n\n\n\n#Predictions in long form that we will eventually shorten\npredictions &lt;- mapped_pred_df |&gt;\n  pivot_longer(\n    cols = starts_with (\"pred\"),\n    names_to = \"model\",\n    values_to = \".pred_F\"\n  ) |&gt;\n  mutate(\n    .pred_C = 1 - .pred_F\n  )\n\npredictions |&gt;\n  select(\n    model, fold, Species, .pred_C, .pred_F\n  )\n\n# A tibble: 553 × 5\n   model            fold  Species .pred_C    .pred_F\n   &lt;chr&gt;            &lt;chr&gt; &lt;fct&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 pred_all         1     C         0.998 0.00155   \n 2 pred_no_disk     1     C         0.990 0.00979   \n 3 pred_no_rays     1     C         0.995 0.00462   \n 4 pred_no_diameter 1     C         0.992 0.00799   \n 5 pred_no_stem     1     C         0.655 0.345     \n 6 pred_2           1     C         0.987 0.0134    \n 7 pred_null        1     C         0.475 0.525     \n 8 pred_all         1     C         1.00  0.00000843\n 9 pred_no_disk     1     C         0.968 0.0316    \n10 pred_no_rays     1     C         1.00  0.000297  \n# ℹ 543 more rows\n\n\nThe code below will give us the average Brier score which we can use measure the accuracy of our predictions.\n\nbrier_all_models &lt;- predictions |&gt;\n  group_by(model, fold) |&gt;\n  brier_class(\n    truth = Species,\n    .pred_C\n  )\n\nbrier_all_models |&gt;\n  ungroup() |&gt;\n  group_by(model) |&gt;\n  summarize(\n    mean_brier = mean(.estimate),\n    se_brier = sd(.estimate)/sqrt(4) #4 estimates\n  ) |&gt;\n  arrange(mean_brier)\n\n# A tibble: 7 × 3\n  model            mean_brier se_brier\n  &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n1 pred_all             0.0729  0.0258 \n2 pred_no_disk         0.0781  0.0162 \n3 pred_no_diameter     0.0807  0.0210 \n4 pred_2               0.0813  0.0189 \n5 pred_no_rays         0.0981  0.0244 \n6 pred_no_stem         0.122   0.0188 \n7 pred_null            0.255   0.00255\n\n\nBased on the data collection, the factors I chose to put in my models seem to display the most differences between species. When creating the models, I wanted it to be able to predict as well as it could. I started with all of the predictors, then did different model with all of them but one. In this case, we want the model that has the lowest mean brier score, which was using all of the factors. Now, what we do is we fit the final model and test using that model.\n\nencelia_final_glm &lt;- glm(Species ~ number_rays + disk_diameter + ray_diameter + stem_length, data = encelia_train, family = \"binomial\")\n\n\nencelia_prediction &lt;- encelia_final_glm |&gt;\n  augment(newdata = encelia_test, type.predict = \"response\") |&gt;\n  mutate(\n    .pred_F = .fitted,\n    .pred_C = 1 - .pred_F,\n    .pred_species = if_else(.fitted &gt;= 0.5, \"F\", \"C\")|&gt;\n      as.factor()\n  )\n\nencelia_prediction |&gt;\n  select(Species, .pred_species, .pred_C, .pred_F)\n\n# A tibble: 21 × 4\n   Species .pred_species   .pred_C     .pred_F\n   &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n 1 F       F             0.00224   0.998      \n 2 C       C             1.00      0.000289   \n 3 C       F             0.0000531 1.00       \n 4 F       F             0.000784  0.999      \n 5 C       C             1.00      0.000000154\n 6 C       C             0.987     0.0133     \n 7 F       F             0.0000143 1.00       \n 8 F       F             0.000376  1.00       \n 9 F       F             0.0000747 1.00       \n10 C       F             0.225     0.775      \n# ℹ 11 more rows"
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#insights",
    "href": "Project-Model-Selection-Encelia.html#insights",
    "title": "Encelia Data",
    "section": "Insights",
    "text": "Insights\n\nencelia_prediction |&gt;\n  conf_mat(\n    truth = Species, \n    estimate = .pred_species\n  )\n\n          Truth\nPrediction C F\n         C 5 0\n         F 3 9\n\n\nThe confusion matrix above tells me if my model is predicting correctly. The ones that are in the place of having double of the letter (top left and bottom right) mean a correct prediction, while if they are in conflicting letter places (top right and bottom left), it is an incorrect prediction. My model is making pretty accurate predictions for the Encelia farinosa plant, but not as accurate predictions for the Encelia californica plant. The model got 14 out of 17 predictions correct.\nThere were three flowers that were incorrectly predicted. I think they might be incorrectly predicted because they had measurements that were abnormally like the Encelia farinosa plant rather than the rest of the Encelia californica. There was a lot of overlap in some of the measurements where it could have been a toss-up to which plant it was if they had not already been identified.\nOverall, the model was pretty accurate and could be used to distinguish which Encelia species a plant is based on the flower’s disk diameter, ray diameter, ray count, and stem length."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "Project1.html",
    "href": "Project1.html",
    "title": "Project 1",
    "section": "",
    "text": "Motivation and Context\nFor this project, I looked into the hospital dataset that we worked with throughout the semester. When we observed this data, the biggest questions I had in my head were these: how did we get to the final numbers of recommendation or overall score? Did certain aspects have more effects on the scores over others?\n\n\nDesign and Data Collection\nThis dataset is a list of hospital rating for the Hospital Consumer Assessment of Healthcare Providers and Systems (HCAHPS), which is a survey for patients about their most recent hospital stay. The questions asked about many variables including: cleanliness, nurse communication, doctor communication, staff responsiveness, communication about medicine, discharge information, care transition, and quietness. These variables are the explanatory variable for our model. An overall score and a recommendation score was also collected, which will be our response variable for the model. These scores were collected in three different measurements: star ratings, linear mean scores, and with descriptions. The only scoring system that was a little difficult to understand was the star ratings. According to the data dictionary, “The Overall Star Ratings are designed to assist patients, consumers, and others in comparing hospitals side-by-side… The hospitals can receive between one and five stars, with five stars being the highest rating, and the more stars, the better the hospital performs on the quality measures.”\nThis data was collected through a random sample of eligible discharges from a hospital. The eligibility includes:\n\nat least 18 years of age at time of admission,\nat least one overnight stay, and\nnon-psychiatric MS-DRG/principal diagnosis at discharge.\n\nThese patients were sampled randomly through telephone, IVR modes, and mail.\n\n\nMain Objective\nMy goal for this project is to see what factors are the heaviest influences on the overall and recommended scores that are reported using a linear regression model. I want to see which of the explanatory variables has the most impact.\nWhile linear regression models are not always the best model that can be used to predict what factor has the highest influence on an outcome, I had been learning about them in Math 435 and wanted to combine that knowledge with coding to see what the outcome would be. A logistic regression model would have made much more sense for predicting as that is what they are mainly used for.\n\nMy Process\nThe code below reads in the data as well as any packages that are necessary for the data cleaning I did.\n\nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nhospital &lt;- read_csv(\"Data/hospital.csv\", na=c(\"Not Applicable\", \"Not Available\"))\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 443517 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): Facility ID, Facility Name, Address, City/Town, State, ZIP Code, C...\ndbl  (6): Patient Survey Star Rating, Patient Survey Star Rating Footnote, H...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe removed any of the data that said “Not Applicable” or “Not Available” and replaced it with a N/A that would help for further data analysis.\nBelow is the code that I used to only keep the pieces of data that I thought would be relevant to my project.\n\nhospital &lt;- hospital |&gt;\n  select(\n    `Facility ID`,\n    `Facility Name`, \n    `HCAHPS Answer Description`, \n    `Patient Survey Star Rating`, \n     `HCAHPS Linear Mean Value`,\n    `HCAHPS Answer Percent`,\n    `Survey Response Rate Percent`,\n    `Number of Completed Surveys`\n  )\n\nThis renames the long descriptions and makes them shorter for easier coding in the future.\n\nhospital_clean &lt;- hospital |&gt;\n  rename(\n    answer = `HCAHPS Answer Description`,\n    star_rating = `Patient Survey Star Rating`,\n    mean_value = `HCAHPS Linear Mean Value`,\n    answer_percent = `HCAHPS Answer Percent`,\n    fac_id = `Facility ID`,\n    fac_name = `Facility Name`, \n    response_rate = `Survey Response Rate Percent`,\n    surveys = `Number of Completed Surveys`\n  )\n\nI want to group by rating system: star rating, linear mean value, and answer percent.\n\nhospital.star &lt;- hospital_clean |&gt;\n  select(\n    fac_id,\n    fac_name, \n    answer, \n    star_rating, \n    response_rate,\n    surveys\n  )|&gt;\n  filter( \n    !is.na(star_rating)\n  )\n\nhospital.lmv &lt;- hospital_clean |&gt;\n  select(\n    fac_id,\n    fac_name, \n    answer, \n    mean_value, \n    response_rate, \n    surveys\n  )|&gt;\n  filter( \n    !is.na(mean_value)\n  )\n\nhospital.answer &lt;- hospital_clean |&gt;\n   select(\n    fac_id,\n    fac_name, \n    answer, \n    answer_percent, \n    response_rate,\n    surveys\n  )|&gt;\n  filter( \n    !is.na(answer_percent))\n\nThis is trying to pivot wider so that we can visualize the data in a different way that is easier to read in columns rather than multiple rows.\n\nhospital.lmv &lt;- pivot_wider(hospital.lmv, names_from = answer, values_from = mean_value)\nhospital.answer &lt;- pivot_wider(hospital.answer, names_from = answer, values_from = answer_percent)\nhospital.star &lt;- pivot_wider(hospital.star, names_from = answer, values_from = star_rating)\n\nIn looking at the data, the more specific data set would be the linear mean score, hence, I will be continuing to observe with just that data set.\nThe code below renames the pivoted variables to shorten them for further coding ease.\n\nhospital.lmv &lt;- hospital.lmv |&gt;\n  rename(\n      nurse_com = `Nurse communication - linear mean score`,\n      doc_com = `Doctor communication - linear mean score`,\n      staff_response = `Staff responsiveness - linear mean score`,\n      med_coms = `Communication about medicines - linear mean score`, \n      discharge_info = `Discharge information - linear mean score`,\n      care_transition = `Care transition - linear mean score`,\n      clean = `Cleanliness - linear mean score`,\n      quiet = `Quietness - linear mean score`,\n      overall = `Overall hospital rating - linear mean score`,\n      rec = `Recommend hospital - linear mean score`\n  )\n\nhospital.star &lt;- hospital.star |&gt;\n  rename(\n      nurse_com = `Nurse communication - star rating`,\n      doc_com = `Doctor communication - star rating`,\n      staff_response = `Staff responsiveness - star rating`,\n      med_coms = `Communication about medicines - star rating`, \n      discharge_info = `Discharge information - star rating`,\n      care_transition = `Care transition - star rating`,\n      clean = `Cleanliness - star rating`,\n      quiet = `Quietness - star rating`,\n      overall = `Overall hospital rating - star rating`,\n      rec = `Recommend hospital - star rating`\n  )\n\nThis gets the average of the scores that could affect the overall or recommended score and creates a new column for that data point for each hospital.\n\nhospital.lmv &lt;- hospital.lmv |&gt;\n  mutate( \n    average_score = rowMeans(subset(hospital.lmv, select = c(nurse_com, doc_com, staff_response, med_coms, discharge_info, care_transition, clean, quiet)), na.rm = TRUE)\n  )\n\nThe code below finds the differences in the overall and recommendation scores versus the calculated average score.\n\nhospital.lmv &lt;- hospital.lmv |&gt;\n  mutate(\n    o.a = overall - average_score, \n    r.a = rec - average_score\n  )\nhead(hospital.lmv)\n\n# A tibble: 6 × 17\n  fac_id fac_name         response_rate surveys nurse_com doc_com staff_response\n  &lt;chr&gt;  &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1 010001 SOUTHEAST HEALT…            18     684        89      91             80\n2 010005 MARSHALL MEDICA…            16     667        90      92             77\n3 010006 NORTH ALABAMA M…            18    1642        89      89             76\n4 010007 MIZELL MEMORIAL…            23     159        94      95             89\n5 010011 ST. VINCENT'S E…            23    1026        90      90             83\n6 010012 DEKALB REGIONAL…            16     302        93      93             85\n# ℹ 10 more variables: med_coms &lt;dbl&gt;, discharge_info &lt;dbl&gt;,\n#   care_transition &lt;dbl&gt;, clean &lt;dbl&gt;, quiet &lt;dbl&gt;, overall &lt;dbl&gt;, rec &lt;dbl&gt;,\n#   average_score &lt;dbl&gt;, o.a &lt;dbl&gt;, r.a &lt;dbl&gt;\n\n\nThe section above does not do too much for the initial goal of the project, but it is an interesting thing to see how the overall and recommended scores compare to the average that was calculated from the variables.\nOne thing I noticed was that the average was mostly lower than the overall score. Only 101 out of the 3197 entries had an average that was higher than the overall score. 19 entries had no difference in their average and overall score.\nNow, what I want to find is which of the variables has the most impact on the recommended and overall scores, and I will do this through a multiple linear regression model.\nThis is my linear regression model for the given overall score using all of the factors.\n\noverall.lm &lt;- lm(overall ~ nurse_com + doc_com +  staff_response + med_coms + discharge_info + care_transition + clean + quiet, data = hospital.lmv)\n\nsummary(overall.lm)\n\n\nCall:\nlm(formula = overall ~ nurse_com + doc_com + staff_response + \n    med_coms + discharge_info + care_transition + clean + quiet, \n    data = hospital.lmv)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7876  -0.8752   0.0497   0.9688   5.3160 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -17.420251   1.284799 -13.559  &lt; 2e-16 ***\nnurse_com         0.375618   0.026356  14.252  &lt; 2e-16 ***\ndoc_com           0.131559   0.018765   7.011 2.88e-12 ***\nstaff_response    0.030552   0.011555   2.644 0.008231 ** \nmed_coms         -0.041388   0.011441  -3.618 0.000302 ***\ndischarge_info    0.038515   0.011190   3.442 0.000585 ***\ncare_transition   0.514732   0.019467  26.442  &lt; 2e-16 ***\nclean             0.100597   0.007418  13.561  &lt; 2e-16 ***\nquiet             0.074207   0.006587  11.267  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.456 on 3188 degrees of freedom\nMultiple R-squared:  0.8596,    Adjusted R-squared:  0.8592 \nF-statistic:  2440 on 8 and 3188 DF,  p-value: &lt; 2.2e-16\n\nplot(overall.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow is my linear regression model for the given reccommended score using all of the variables.\n\nrec.lm &lt;- lm(rec ~ nurse_com + doc_com + staff_response + med_coms + discharge_info + care_transition + clean + quiet, data = hospital.lmv)\n\nsummary(rec.lm)\n\n\nCall:\nlm(formula = rec ~ nurse_com + doc_com + staff_response + med_coms + \n    discharge_info + care_transition + clean + quiet, data = hospital.lmv)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.6046 -1.5158  0.1297  1.6339  6.7448 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -49.95953    2.07335 -24.096  &lt; 2e-16 ***\nnurse_com         0.51411    0.04253  12.088  &lt; 2e-16 ***\ndoc_com           0.15841    0.03028   5.231 1.79e-07 ***\nstaff_response   -0.05011    0.01865  -2.688  0.00724 ** \nmed_coms         -0.12499    0.01846  -6.770 1.53e-11 ***\ndischarge_info    0.02572    0.01806   1.424  0.15441    \ncare_transition   0.95662    0.03141  30.451  &lt; 2e-16 ***\nclean             0.08131    0.01197   6.793 1.31e-11 ***\nquiet             0.03342    0.01063   3.144  0.00168 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.349 on 3188 degrees of freedom\nMultiple R-squared:  0.7812,    Adjusted R-squared:  0.7807 \nF-statistic:  1423 on 8 and 3188 DF,  p-value: &lt; 2.2e-16\n\nplot(rec.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn reading the summaries and looking at the residual plots, I noticed a few things:\nFirstly, the linear model for the overall scores is generally a good fit. However, the recommendation score is not a great fit. There are four plots that we look at for determining if a linear model is a good fit. Those plots are Residuals vs. Fitted, Q-Q residuals, Scale-Location, and Residuals vs. Leverage. To make things short, we want certain shapes for each of these graphs to determine if the model is a good fit. For the Residuals vs. Fitted, Scale-Location, and Residuals vs. Leverage graphs, we want them generally to be scattered about a horizontal line. The Q-Q residuals is different in the sense that we want a diagonal line. For the Residuals vs. Leverage graph, we want our data to form a sort of cone shape that is limited by Cook’s distance. Data that is outside that distance tends to skew the model. Notice how neither of the models really go past that line.\nSecondly, there are a few data points that stick out and are labeled on the graphs. I did go back through the data to observe those, and they did have higher scores than anticipated, but I chalked it up to great experiences at the hospital.\nThirdly, we look at the r-squared values for each of the models in their summaries. Typically, the larger the value, the more precise the predictor variables are able to predict the overall or recommended score. With this knowledge, the overall linear model is a better model than the recommended model (0.8596 vs. 0.7812).\nLastly, I looked through the values for the predictive equation. The equations are as follows:\n\noverall = -17.42 + 0.38(nurse_com) + 0.13(doc_com) + 0.03(staff_response) - 0.04(med_coms) + 0.04 (discharge_info) + 0.51(care_transition) + 0.10(clean) + 0.07(quiet)\nrec = -49.96 + 0.51(nurse_com) + 0.16(doc_com) - 0.05 (staff_response) - 0.12(med_coms) + 0.03 (discharge_info) + 0.96(care_transition) + 0.08(clean) + 0.03(quiet)\n\nI want to now take a few data points and test them on the models. I randomly selected 10 hospitals to use as my testers.\n\nhospital_testers &lt;- sample(nrow(hospital.lmv), 10)\n\nThese numbers will be different each time the code is run, but when I sampled mine, the rows chosen were as follows:\n\nhospital_testers &lt;- c(296, 190, 1857, 1686, 194, 1172, 2281, 1745, 97, 1226)\n\nhospital_test &lt;- hospital.lmv[hospital_testers,]\nhospital_test\n\n# A tibble: 10 × 17\n   fac_id fac_name        response_rate surveys nurse_com doc_com staff_response\n   &lt;chr&gt;  &lt;chr&gt;                   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n 1 050278 PROVIDENCE HOL…            15     341        92      90             85\n 2 050030 OROVILLE HOSPI…            14     495        82      84             68\n 3 310119 THE UNIVERSITY…             7     699        85      88             71\n 4 26023F COLUMBIA MO VA…            37     428        94      92             86\n 5 050040 LAC/OLIVE VIEW…            10     384        89      90             78\n 6 180004 OWENSBORO HEAL…            31     250        94      93             86\n 7 370097 SOUTHWESTERN M…            15     278        88      88             83\n 8 281341 CHADRON COMMUN…            30     100        91      97             89\n 9 030078 PHOENIX INDIAN…            22     188        93      94             87\n10 180141 UNIVERSITY OF …            16    1653        87      87             78\n# ℹ 10 more variables: med_coms &lt;dbl&gt;, discharge_info &lt;dbl&gt;,\n#   care_transition &lt;dbl&gt;, clean &lt;dbl&gt;, quiet &lt;dbl&gt;, overall &lt;dbl&gt;, rec &lt;dbl&gt;,\n#   average_score &lt;dbl&gt;, o.a &lt;dbl&gt;, r.a &lt;dbl&gt;\n\n\nIn trying to write a function that I could use to plug in the values, my code didn’t seem to work, so I will be doing this the long way with plugging in the values individually. They will be organized by one word in the facility name as the name of the variable with an o or an r after depending on which prediction it is for.\n\nOverall Predictions\n\nprovidence.o = -17.42 + 0.38*92 + 0.13*90 + 0.03*85 - 0.04*74 + 0.04*87 + 0.51*82 + 0.10*88 + 0.07*75 \noroville.o = -17.42 + 0.38*82 + 0.13*84 + 0.03*68 - 0.04*65 + 0.04*78 + 0.51*71 + 0.10*75 + 0.07*66\nuniversity.o = -17.42 + 0.38*85 + 0.13*88 + 0.03*71 - 0.04*67 + 0.04*79 + 0.51*76 + 0.10*77 + 0.07*76 \ncolumbia.o = -17.42 + 0.38*94 + 0.13*92 + 0.03*86 - 0.04*83 + 0.04*88 + 0.51*85 + 0.10*91 + 0.07*87 \nolive.o = -17.42 + 0.38*89 + 0.13*90 + 0.03*78 - 0.04*75 + 0.04*87 + 0.51*80 + 0.10*85 + 0.07*74 \nowensboro.o = -17.42 + 0.38*94 + 0.13*93 + 0.03*86 - 0.04*80 + 0.04*89 + 0.51*84 + 0.10*93 + 0.07*87 \nsouthwestern.o = -17.42 + 0.38*88 + 0.13*88 + 0.03*83 - 0.04*73 + 0.04*84 + 0.51*78 + 0.10*87 + 0.07*85 \nchadron.o = -17.42 + 0.38*91 + 0.13*97 + 0.03*89 - 0.04*85 + 0.04*90 + 0.51*81 + 0.10*87 + 0.07*90 \nphoenix.o = -17.42 + 0.38*93 + 0.13*94 + 0.03*87 - 0.04*80 + 0.04*85 + 0.51*80 + 0.10*85 + 0.07*85 \nlouisville.o = -17.42 + 0.38*87 + 0.13*87 + 0.03*78 - 0.04*71 + 0.04*85 + 0.51*77 + 0.10*84 + 0.07*74 \n\n\n\n\n\n\n\n\n\nHospital\nPredicted Overall\nActual Overall\n\n\n\n\nProvidence Holy Cross Medical Center\n88.18\n90\n\n\nOroville Hospital\n75.55\n74\n\n\nThe University Hospital\n80.71\n81\n\n\nColumbia Mo Va Medical Center\n91.58\n93\n\n\nLAC/Olive View-UCLA Medical Center\n85.4\n86\n\n\nOwensboro Health Muhlenberg Community Hospital\n91.56\n93\n\n\nSouthwestern Medical Center\n84.82\n86\n\n\nChadron Community Hospital and Health Services\n88.95\n90\n\n\nPhoenix Indian Medical Center\n88.2\n90\n\n\nUniversity of Louisville Hospital\n82.7\n84\n\n\n\nThe predictions are very close to the actual score it received in this case.\n\n\nRecommended Predictions\n\nprovidence.r = -49.96 + 0.51*92 + 0.16*90 - 0.05*85 - 0.12*74 + 0.03*87 + 0.96*82 + 0.08*88 + 0.03*75 \noroville.r = -49.96 + 0.51*82 + 0.16*84 - 0.05*68 - 0.12*65 + 0.03*78 + 0.96*71 + 0.08*75 + 0.03*66 \nuniversity.r = -49.96 + 0.51*85 + 0.16*88 - 0.05*71 - 0.12*67 + 0.03*79 + 0.96*76 + 0.08*77 + 0.03*76 \ncolumbia.r = -49.96 + 0.51*94 + 0.16*92 - 0.05*86 - 0.12*83 + 0.03*88 + 0.96*85 + 0.08*91 + 0.03*87 \nolive.r = -49.96 + 0.51*89 + 0.16*90 - 0.05*78 - 0.12*75 + 0.03*87 + 0.96*80 + 0.08*85 + 0.03*74 \nowensboro.r = -49.96 + 0.51*94 + 0.16*93 - 0.05*86 - 0.12*80 + 0.03*89 + 0.96*84 + 0.08*93 + 0.03*87 \nsouthwestern.r = -49.96 + 0.51*88 + 0.16*88 - 0.05*83 - 0.12*73 + 0.03*84 + 0.96*78 + 0.08*87 + 0.03*85 \nchadron.r = -49.96 + 0.51*91 + 0.16*97 - 0.05*89 - 0.12*85 + 0.03*90 + 0.96*81 + 0.08*87 + 0.03*90 \nphoenix.r = -49.96 + 0.51*93 + 0.16*94 - 0.05*87 - 0.12*80 + 0.03*85 + 0.96*80 + 0.08*85 + 0.03*85 \nlouisville.r = -49.96 + 0.51*87 + 0.16*87 - 0.05*78 - 0.12*71 + 0.03*85 + 0.96*77 + 0.08*84 + 0.03*74 \n\n\n\n\n\n\n\n\n\nHospital\nPredicted Recommended\nActual Recommended\n\n\n\n\nProvidence Holy Cross Medical Center\n88.85\n88\n\n\nOroville Hospital\n72.58\n69\n\n\nThe University Hospital\n79.65\n79\n\n\nColumbia Mo Va Medical Center\n92.57\n92\n\n\nLAC/Olive View-UCLA Medical Center\n85.36\n86\n\n\nOwensboro Health Muhlenberg Community Hospital\n92.32\n91\n\n\nSouthwestern Medical Center\n83\n85\n\n\nChadron Community Hospital and Health Services\n87.44\n91\n\n\nPhoenix Indian Medical Center\n87.26\n86\n\n\nUniversity of Louisville Hospital\n81.32\n82\n\n\n\nThe recommended score has a little bit more range to the predictions.\n\n\n\n\nConclusion\nI was curious about what variable impacts the scores the most. With these equations, there is a heavy influence from the care transition for both models as it has the highest coefficient, and therefore influence when calculating the final score.\nBoth linear models have pretty accurate predictions with occasional variation when tested with a random sample of hospitals.\n\n\nLimitations\nIf I were to receive this data to help me choose a hospital to go to, I would rely more on the overall scores model to help me make my decision over the recommended score. However, this data should not be the only research done for these hospitals. These models do not always account for every different experience that a person would get, and some may be skewed positive or skewed negative experiences. The response rates for each of the hospitals varies incredibly, which can also be a factor in what data they receive. In looking at the data collection process, it seems that most of the surveys were of a voluntary response, indicating that mostly those who are passionate about their experience will respond. This may not be truly random data and should be taken with a grain of salt. If used in the real world, the model would affect those who were trying to make a decision on what hospital they should use. If someone tried to choose the best hospital and got one that had more skewed results, they might not get the standard of care that they anticipated which could lead to a worse experience. It could also very much rule out great hospitals that got bad reviews from some unfortunate experiences. With this, certain hospitals could get over-crowded if there are a lot of good reviews rating them higher. This in turn could cause for more bad reviews as people wait longer for their care. Overall, this model could have a lot of influence on how these hospitals perform and how the people going to these hospitals could receive their care."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]