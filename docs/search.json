[
  {
    "objectID": "Project-Model-Selection-Encelia.html",
    "href": "Project-Model-Selection-Encelia.html",
    "title": "Encelia Data",
    "section": "",
    "text": "Encelia californica and Encelia farinosa are two bright yellow flowers that can be found in the Fullerton Arboretum. They can be used to restore deserts and native gardens and help with erosion control. It is important that people understand the differences between the two species if they are trying to restore areas and need the correct plant to do so. When restoring areas, it is important that the plant can flourish properly in the environment it is placed in. One of the biggest differences are when they flower. The Encelia farinosa flowers from February to May then August to September while Encelia californica flowers from February to October. Another difference is the elevations and locations where they thrive. Encelia farinosa can go up to 1000 meters above sea level and thrives in more inland desert regions, while the Encelia californica can go up to 600 meters above sea level and is found in more coastal regions.\nThe Fullerton Arboretum focuses on conserving and displaying the beauty of plants in a local community area. The Arboretum works with California State University, Fullerton to provide students with the ability to learn, research and appreciate these plants and horticulture."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#motivation-and-context",
    "href": "Project-Model-Selection-Encelia.html#motivation-and-context",
    "title": "Encelia Data",
    "section": "",
    "text": "Encelia californica and Encelia farinosa are two bright yellow flowers that can be found in the Fullerton Arboretum. They can be used to restore deserts and native gardens and help with erosion control. It is important that people understand the differences between the two species if they are trying to restore areas and need the correct plant to do so. When restoring areas, it is important that the plant can flourish properly in the environment it is placed in. One of the biggest differences are when they flower. The Encelia farinosa flowers from February to May then August to September while Encelia californica flowers from February to October. Another difference is the elevations and locations where they thrive. Encelia farinosa can go up to 1000 meters above sea level and thrives in more inland desert regions, while the Encelia californica can go up to 600 meters above sea level and is found in more coastal regions.\nThe Fullerton Arboretum focuses on conserving and displaying the beauty of plants in a local community area. The Arboretum works with California State University, Fullerton to provide students with the ability to learn, research and appreciate these plants and horticulture."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#main-objective",
    "href": "Project-Model-Selection-Encelia.html#main-objective",
    "title": "Encelia Data",
    "section": "Main Objective",
    "text": "Main Objective\nWith this project, we are using a few main differences of these plants to build a model that can distinguish which plant is which based on a few basic measurements."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#packages-used-in-this-analysis",
    "href": "Project-Model-Selection-Encelia.html#packages-used-in-this-analysis",
    "title": "Encelia Data",
    "section": "Packages Used In This Analysis",
    "text": "Packages Used In This Analysis\n\nlibrary(here)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(rsample)\nlibrary(purrr)\nlibrary(yardstick)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(broom)\n\n\n\n\nPackage\nUse\n\n\n\n\nhere\nto easily load and save data\n\n\nreadr\nto import the CSV file data\n\n\ndplyr\nto massage and summarize data\n\n\nggplot2\nto create nice-looking and informative graphs\n\n\nrsample\nto split data into training and test sets\n\n\npurrr\nto run the cross-validation\n\n\nyardstick\nto evalute the accuracy of the models\n\n\ntidyr\nto “pivot” the predictions data frame so that each row represents 1 model"
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#design-and-data-collection",
    "href": "Project-Model-Selection-Encelia.html#design-and-data-collection",
    "title": "Encelia Data",
    "section": "Design and Data Collection",
    "text": "Design and Data Collection\nAs a class, we went to the Fullerton Arboretum and identified the two flowers that we wanted to collect the data on. We split into pairs and collected four pieces of data on the two species: the number of rays on each flower, the diameter of the center disk, the diameter of the entire flower, and the stem length. Everything was measured in centimeters. We used a simple measuring ruler and recorded our data on a sheet of paper. The number of rays was counted individually. The diameter of the disk was measured through the center from end to end. The diameter of the entire flower was measured similarly to the disk but extended to include the rays. The stem length was measured from the bottom of the flower part of the plant to the first area that it branched off closest to the flower. Each pair measured 10 flowers of each species, so in total we got 50 of each species.\n\nA few limitations of the data collection was limited access. There were many flowers that could not be accessed due to the desire to respect the grounds and not create new and unnatural paths. A lot of the flowers were also close to more dangerous plants like cacti or in areas that were fenced off, so that limited the randomness of the collection. My partner and I decided to stay consistent with who was collecting the data. Since we did this, we tried to limit the variation in measuring judgement that may have occurred had we not stayed consistent, like deciding where the center of a flower was. Additionally, the measurements that we did were not always as accurate as we wanted them. The rulers only went so far in the parts to a whole so judgement in rounding could be different based on who did the measuring."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#training-test-split",
    "href": "Project-Model-Selection-Encelia.html#training-test-split",
    "title": "Encelia Data",
    "section": "Training-Test Split",
    "text": "Training-Test Split\nI’m going to read in the data and display it below.\n\nlibrary(readr)\nencelia &lt;- read_csv(\"Data/Encelia Classification Data Collection - Sheet1.csv\")\n\nRows: 100 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Species\ndbl (4): number_rays, disk_diameter, ray_diameter, stem_length\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nencelia &lt;- encelia|&gt;\n  mutate(\n    Species = Species |&gt;\n      as.factor()\n  )\n\ncontrasts(encelia$Species)\n\n  F\nC 0\nF 1\n\n\nThe data consists of numeric values which are the measurements of each variable in centimeters or whole numbers for the ray count, so we do not need to change anything.\n\nset.seed(3)\nencelia_split &lt;- initial_split(\n  encelia,\n  strata = Species,\n  prop = 0.80\n)\n\nencelia_train &lt;- training(encelia_split)\nencelia_test &lt;- testing(encelia_split)\n\nencelia_train$Species &lt;- as.factor(encelia_train$Species)\n\n#This creates the folds for later.\nencelia_cv &lt;- encelia_train |&gt;\n  vfold_cv(\n    v=4, #4 fold cross-validation\n    repeats = 1 #gives one unique prediction per observation in the training set\n  )\n\nA training set trains the model to be able to predict the outcome, and the test set will be put into the model to see how well it actually does predict. It will be useful for this objective because we want to build a model that can predict what species a flower is given the measurements. Additionally, I created the folds in the set that we can use to get the predictions."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#exploratory-data-analysis",
    "href": "Project-Model-Selection-Encelia.html#exploratory-data-analysis",
    "title": "Encelia Data",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nDuring the Logistic Regression with Encelia activity, we created plots that compared each of the variables to the species that it was. We knew that there were some differences to the flowers, and we were able to pinpoint the main identifiers that were different by comparing the two species. The graphs will help to identify which variables we should consider choosing for our model.\n\n#disk diameter vs. species\nggplot(\n  data = encelia_train,\n  mapping = aes(x = disk_diameter, y=Species)\n  ) + \n  geom_jitter(height = 0.1)\n\n\n\n\n\n\n\n\nThe disk diameter is fairly similar for both flowers so it might not be as useful when creating our model. The model might get confused since there is not a significant difference.\n\n#number of rays vs. species\nggplot(\n  data = encelia_train,\n  mapping = aes(x = number_rays, y=Species)\n  ) + \n  geom_jitter(height = 0.1)\n\n\n\n\n\n\n\n\nWe can see that the Encelia californica tended to have a higher number of rays than the Encelia farinosa, which is a known identifier for these species.\n\n#ray diameter vs. species\nggplot(\n  data = encelia_train,\n  mapping = aes(x = ray_diameter, y=Species)\n  ) + \n  geom_jitter(height = 0.1)\n\n\n\n\n\n\n\n\nThe ray diameter seems very similar for both flowers, but the average for the Encelia californica was higher overall.\n\n#stem length vs. species\nggplot(\n  data = encelia_train,\n  mapping = aes(x = stem_length, y=Species)\n  ) + \n  geom_jitter(height = 0.1)\n\nWarning: Removed 16 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nStem length for the Encelia californica had a wider range than the Encelia farinosa. With this knowledge, we could see that most of the Encelia farinosa had overall smaller stem lengths which could help us identify for our model.\nAfter looking at these plots, we can see which variables might be the most important when helping our model predict the species. Before looking at the modeling, it seems like the stem length and ray diameter might be the biggest factors in predicting which species a flower is."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#modeling",
    "href": "Project-Model-Selection-Encelia.html#modeling",
    "title": "Encelia Data",
    "section": "Modeling",
    "text": "Modeling\nLogistic regression is a machine model that attempts to distinguish between categories and is used for prediction. The goal of our project is to be able to distinguish which flower is which so this model will help us do that.\nCross-validation is important to see how well the model works to predict what we want. It involves separating the data into training and testing sets. The training set trains the model using that data, and the testing set tests if the model really works how it should.\n\nencelia_prediction &lt;- function(split) {\n\n#Step 1: create the training and validation sets \ntrain &lt;- analysis(split)\nvalid &lt;- assessment(split)\n\n#Step 2: Fit all the models on the training set\nencelia_glm1 &lt;- glm(Species ~ number_rays + disk_diameter + ray_diameter + stem_length, data = train, family = \"binomial\")\nencelia_glm2 &lt;- glm(Species ~ number_rays + ray_diameter + stem_length, data = train, family = \"binomial\")\nencelia_glm3 &lt;- glm(Species ~ disk_diameter + ray_diameter + stem_length, data = train, family = \"binomial\")\nencelia_glm4 &lt;- glm(Species ~ number_rays + disk_diameter + stem_length, data = train, family = \"binomial\")\nencelia_glm5 &lt;- glm(Species ~ number_rays + disk_diameter + ray_diameter, data = train, family = \"binomial\")\nencelia_glm6 &lt;- glm(Species ~ number_rays + stem_length, data = train, family = \"binomial\")\nencelia_null &lt;- glm(Species ~1, data = train, family = \"binomial\")\n\n#Step 3: Make all the predictions on the validation set\nvalid_predictions &lt;- valid |&gt;\n  mutate(\n    pred_all = predict(encelia_glm1, newdata = valid, type = \"response\"), \n    pred_no_disk = predict(encelia_glm2, newdata = valid, type = \"response\"), \n    pred_no_rays = predict(encelia_glm3, newdata = valid, type = \"response\"),\n    pred_no_diameter = predict(encelia_glm4, newdata = valid, type = \"response\"),\n    pred_no_stem = predict(encelia_glm5, newdata = valid, type = \"response\"),\n    pred_2 = predict(encelia_glm6, newdata = valid, type = \"response\"),\n    pred_null = predict(encelia_null, newdata = valid, type = \"response\")\n  )\nreturn(valid_predictions)\n}\n\n\n#This runs the function on the splits.\nmapped_pred &lt;- map(\n  encelia_cv$splits, \n  encelia_prediction\n)\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nmapped_pred_df &lt;- mapped_pred |&gt;\n  bind_rows(\n    .id = \"fold\"\n  )\n\nmapped_pred_df |&gt;\n  select(\n    fold, Species, pred_all, pred_no_disk, pred_no_rays, pred_no_diameter, pred_no_stem, pred_2, pred_null\n  )\n\n# A tibble: 79 × 9\n   fold  Species  pred_all pred_no_disk pred_no_rays pred_no_diameter\n   &lt;chr&gt; &lt;fct&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;\n 1 1     C        1.55e- 3      9.79e-3     4.62e- 3         7.99e- 3\n 2 1     C        8.43e- 6      3.16e-2     2.97e- 4         1.29e- 3\n 3 1     C        1.46e- 2      2.99e-2     2.73e- 5         4.59e- 5\n 4 1     C        4.52e- 5      5.60e-4     2.63e- 5         5.21e- 4\n 5 1     C       NA            NA          NA               NA       \n 6 1     C       NA            NA          NA               NA       \n 7 1     C       NA            NA          NA               NA       \n 8 1     C        1.68e- 5      1.60e-5     6.56e- 4         5.55e- 2\n 9 1     C        2.22e-16      6.17e-9     2.45e-12         2.02e-11\n10 1     C        2.59e- 3      1.28e-2     7.08e- 1         4.76e- 1\n# ℹ 69 more rows\n# ℹ 3 more variables: pred_no_stem &lt;dbl&gt;, pred_2 &lt;dbl&gt;, pred_null &lt;dbl&gt;\n\n\n\n#Predictions in long form that we will eventually shorten\npredictions &lt;- mapped_pred_df |&gt;\n  pivot_longer(\n    cols = starts_with (\"pred\"),\n    names_to = \"model\",\n    values_to = \".pred_F\"\n  ) |&gt;\n  mutate(\n    .pred_C = 1 - .pred_F\n  )\n\npredictions |&gt;\n  select(\n    model, fold, Species, .pred_C, .pred_F\n  )\n\n# A tibble: 553 × 5\n   model            fold  Species .pred_C    .pred_F\n   &lt;chr&gt;            &lt;chr&gt; &lt;fct&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 pred_all         1     C         0.998 0.00155   \n 2 pred_no_disk     1     C         0.990 0.00979   \n 3 pred_no_rays     1     C         0.995 0.00462   \n 4 pred_no_diameter 1     C         0.992 0.00799   \n 5 pred_no_stem     1     C         0.655 0.345     \n 6 pred_2           1     C         0.987 0.0134    \n 7 pred_null        1     C         0.475 0.525     \n 8 pred_all         1     C         1.00  0.00000843\n 9 pred_no_disk     1     C         0.968 0.0316    \n10 pred_no_rays     1     C         1.00  0.000297  \n# ℹ 543 more rows\n\n\nThe code below will give us the average Brier score which we can use measure the accuracy of our predictions.\n\nbrier_all_models &lt;- predictions |&gt;\n  group_by(model, fold) |&gt;\n  brier_class(\n    truth = Species,\n    .pred_C\n  )\n\nbrier_all_models |&gt;\n  ungroup() |&gt;\n  group_by(model) |&gt;\n  summarize(\n    mean_brier = mean(.estimate),\n    se_brier = sd(.estimate)/sqrt(4) #4 estimates\n  ) |&gt;\n  arrange(mean_brier)\n\n# A tibble: 7 × 3\n  model            mean_brier se_brier\n  &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n1 pred_all             0.0729  0.0258 \n2 pred_no_disk         0.0781  0.0162 \n3 pred_no_diameter     0.0807  0.0210 \n4 pred_2               0.0813  0.0189 \n5 pred_no_rays         0.0981  0.0244 \n6 pred_no_stem         0.122   0.0188 \n7 pred_null            0.255   0.00255\n\n\nBased on the data collection, the factors I chose to put in my models seem to display the most differences between species. When creating the models, I wanted it to be able to predict as well as it could. I started with all of the predictors, then did different model with all of them but one. In this case, we want the model that has the lowest mean brier score, which was using all of the factors. Now, what we do is we fit the final model and test using that model.\n\nencelia_final_glm &lt;- glm(Species ~ number_rays + disk_diameter + ray_diameter + stem_length, data = encelia_train, family = \"binomial\")\n\n\nencelia_prediction &lt;- encelia_final_glm |&gt;\n  augment(newdata = encelia_test, type.predict = \"response\") |&gt;\n  mutate(\n    .pred_F = .fitted,\n    .pred_C = 1 - .pred_F,\n    .pred_species = if_else(.fitted &gt;= 0.5, \"F\", \"C\")|&gt;\n      as.factor()\n  )\n\nencelia_prediction |&gt;\n  select(Species, .pred_species, .pred_C, .pred_F)\n\n# A tibble: 21 × 4\n   Species .pred_species   .pred_C     .pred_F\n   &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n 1 F       F             0.00224   0.998      \n 2 C       C             1.00      0.000289   \n 3 C       F             0.0000531 1.00       \n 4 F       F             0.000784  0.999      \n 5 C       C             1.00      0.000000154\n 6 C       C             0.987     0.0133     \n 7 F       F             0.0000143 1.00       \n 8 F       F             0.000376  1.00       \n 9 F       F             0.0000747 1.00       \n10 C       F             0.225     0.775      \n# ℹ 11 more rows"
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#insights",
    "href": "Project-Model-Selection-Encelia.html#insights",
    "title": "Encelia Data",
    "section": "Insights",
    "text": "Insights\n\nencelia_prediction |&gt;\n  conf_mat(\n    truth = Species, \n    estimate = .pred_species\n  )\n\n          Truth\nPrediction C F\n         C 5 0\n         F 3 9\n\n\nThe confusion matrix above tells me if my model is predicting correctly. The ones that are in the place of having double of the letter (top left and bottom right) mean a correct prediction, while if they are in conflicting letter places (top right and bottom left), it is an incorrect prediction. My model is making pretty accurate predictions for the Encelia farinosa plant, but not as accurate predictions for the Encelia californica plant. The model got 14 out of 17 predictions correct.\nThere were three flowers that were incorrectly predicted. I think they might be incorrectly predicted because they had measurements that were abnormally like the Encelia farinosa plant rather than the rest of the Encelia californica. There was a lot of overlap in some of the measurements where it could have been a toss-up to which plant it was if they had not already been identified."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "Project1.html",
    "href": "Project1.html",
    "title": "Project 1",
    "section": "",
    "text": "Motivation and Context\nWhile the objectives were described, they were not connected back to the domain context, and there was no explanation of what any of the variables of interest meant.\nFor this project, I looked into the hospital dataset that we worked with throughout the semester. When we observed this data, the biggest questions I had in my head were these: how did we get to the final numbers of recommendation or overall score? Did certain aspects have more effects on the scores over others?\nThis dataset comes from\n\n##This reads in the data as well as any packages that are necessary. \nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nhospital &lt;- read_csv(\"Data/hospital.csv\", na=c(\"Not Applicable\", \"Not Available\"))\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 443517 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): Facility ID, Facility Name, Address, City/Town, State, ZIP Code, C...\ndbl  (6): Patient Survey Star Rating, Patient Survey Star Rating Footnote, H...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#We removed any of the data that said \"Not Applicable\" or \"Not Available\" and replaced it with a N/A that would help for further data analysis. \n\nAlthough I would have liked the comments to have been moved outside the code chunks and into text, you explained both what steps were taken and why you were taking those steps.\n\n##This is to only keeping what I find relevant in my dataset\nhospital &lt;- hospital |&gt;\n  select(\n    `Facility ID`,\n    `Facility Name`, \n    `HCAHPS Answer Description`, \n    `Patient Survey Star Rating`, \n     `HCAHPS Linear Mean Value`,\n    `HCAHPS Answer Percent`,\n    `Survey Response Rate Percent`,\n    `Number of Completed Surveys`\n  )\n\n\n##clean the data & rename things\nhospital_clean &lt;- hospital |&gt;\n  rename(\n    answer = `HCAHPS Answer Description`,\n    star_rating = `Patient Survey Star Rating`,\n    mean_value = `HCAHPS Linear Mean Value`,\n    answer_percent = `HCAHPS Answer Percent`,\n    fac_id = `Facility ID`,\n    fac_name = `Facility Name`, \n    response_rate = `Survey Response Rate Percent`,\n    surveys = `Number of Completed Surveys`\n  )\n\n\n\nMain Objective\nMy goal for this project is to see what factors are the heaviest influences on the overall and recommended scores that are reported.\n\nMy Process\nI want to group by rating system: star rating, linear mean value, and answer percent.\n\n#This groups the data by the rating systems: star, linear mean value, and answer percent. \nhospital.star &lt;- hospital_clean |&gt;\n  select(\n    fac_id,\n    fac_name, \n    answer, \n    star_rating, \n    response_rate,\n    surveys\n  )|&gt;\n  filter( \n    !is.na(star_rating)\n  )\n\nhospital.lmv &lt;- hospital_clean |&gt;\n  select(\n    fac_id,\n    fac_name, \n    answer, \n    mean_value, \n    response_rate, \n    surveys\n  )|&gt;\n  filter( \n    !is.na(mean_value)\n  )\n\nhospital.answer &lt;- hospital_clean |&gt;\n   select(\n    fac_id,\n    fac_name, \n    answer, \n    answer_percent, \n    response_rate,\n    surveys\n  )|&gt;\n  filter( \n    !is.na(answer_percent))\n\n\n##label: This is trying to pivot wider so that we can visualize the data in a different way. \nhospital.lmv &lt;- pivot_wider(hospital.lmv, names_from = answer, values_from = mean_value)\nhospital.answer &lt;- pivot_wider(hospital.answer, names_from = answer, values_from = answer_percent)\nhospital.star &lt;- pivot_wider(hospital.star, names_from = answer, values_from = star_rating)\n\nIn looking at the data, the more specific data set would be the linear mean score, hence, I will be continuing to observe with just that data set.\nThe code below renames the variables to shorten them for further coding ease.\n\nhospital.lmv &lt;- hospital.lmv |&gt;\n  rename(\n      nurse_com = `Nurse communication - linear mean score`,\n      doc_com = `Doctor communication - linear mean score`,\n      staff_response = `Staff responsiveness - linear mean score`,\n      med_coms = `Communication about medicines - linear mean score`, \n      discharge_info = `Discharge information - linear mean score`,\n      care_transition = `Care transition - linear mean score`,\n      clean = `Cleanliness - linear mean score`,\n      quiet = `Quietness - linear mean score`,\n      overall = `Overall hospital rating - linear mean score`,\n      rec = `Recommend hospital - linear mean score`\n  )\n\nhospital.star &lt;- hospital.star |&gt;\n  rename(\n      nurse_com = `Nurse communication - star rating`,\n      doc_com = `Doctor communication - star rating`,\n      staff_response = `Staff responsiveness - star rating`,\n      med_coms = `Communication about medicines - star rating`, \n      discharge_info = `Discharge information - star rating`,\n      care_transition = `Care transition - star rating`,\n      clean = `Cleanliness - star rating`,\n      quiet = `Quietness - star rating`,\n      overall = `Overall hospital rating - star rating`,\n      rec = `Recommend hospital - star rating`\n  )\n\nThis gets the average of the scores that could affect the overall or recommended score and creates a new column for that data point for each hospital.\n\nhospital.lmv &lt;- hospital.lmv |&gt;\n  mutate( \n    average_score = rowMeans(subset(hospital.lmv, select = c(nurse_com, doc_com, staff_response, med_coms, discharge_info, care_transition, clean, quiet)), na.rm = TRUE)\n  )\n\nThe code below finds the differences in the overall and recommendation scores versus the calculated average score.\n\nhospital.lmv &lt;- hospital.lmv |&gt;\n  mutate(\n    o.a = overall - average_score, \n    r.a = rec - average_score\n  )\n\nThe section above does not do too much for the initial goal of the project, but it is an interesting thing to see how they compare.\nNow, what I want to find is which of the variables has the most impact on the recommended and overall scores, and I will do this through a multiple linear regression model.\nThis is my linear regression model for the given overall score.\n\noverall.lm &lt;- lm(overall ~ nurse_com + doc_com +  staff_response + med_coms + discharge_info + care_transition + clean + quiet, data = hospital.lmv)\n\nsummary(overall.lm)\n\n\nCall:\nlm(formula = overall ~ nurse_com + doc_com + staff_response + \n    med_coms + discharge_info + care_transition + clean + quiet, \n    data = hospital.lmv)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7876  -0.8752   0.0497   0.9688   5.3160 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -17.420251   1.284799 -13.559  &lt; 2e-16 ***\nnurse_com         0.375618   0.026356  14.252  &lt; 2e-16 ***\ndoc_com           0.131559   0.018765   7.011 2.88e-12 ***\nstaff_response    0.030552   0.011555   2.644 0.008231 ** \nmed_coms         -0.041388   0.011441  -3.618 0.000302 ***\ndischarge_info    0.038515   0.011190   3.442 0.000585 ***\ncare_transition   0.514732   0.019467  26.442  &lt; 2e-16 ***\nclean             0.100597   0.007418  13.561  &lt; 2e-16 ***\nquiet             0.074207   0.006587  11.267  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.456 on 3188 degrees of freedom\nMultiple R-squared:  0.8596,    Adjusted R-squared:  0.8592 \nF-statistic:  2440 on 8 and 3188 DF,  p-value: &lt; 2.2e-16\n\nplot(overall.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow is my linear regression model for the given reccommended score.\n\nrec.lm &lt;- lm(rec ~ nurse_com + doc_com + staff_response + med_coms + discharge_info + care_transition + clean + quiet, data = hospital.lmv)\n\nsummary(rec.lm)\n\n\nCall:\nlm(formula = rec ~ nurse_com + doc_com + staff_response + med_coms + \n    discharge_info + care_transition + clean + quiet, data = hospital.lmv)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.6046 -1.5158  0.1297  1.6339  6.7448 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -49.95953    2.07335 -24.096  &lt; 2e-16 ***\nnurse_com         0.51411    0.04253  12.088  &lt; 2e-16 ***\ndoc_com           0.15841    0.03028   5.231 1.79e-07 ***\nstaff_response   -0.05011    0.01865  -2.688  0.00724 ** \nmed_coms         -0.12499    0.01846  -6.770 1.53e-11 ***\ndischarge_info    0.02572    0.01806   1.424  0.15441    \ncare_transition   0.95662    0.03141  30.451  &lt; 2e-16 ***\nclean             0.08131    0.01197   6.793 1.31e-11 ***\nquiet             0.03342    0.01063   3.144  0.00168 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.349 on 3188 degrees of freedom\nMultiple R-squared:  0.7812,    Adjusted R-squared:  0.7807 \nF-statistic:  1423 on 8 and 3188 DF,  p-value: &lt; 2.2e-16\n\nplot(rec.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn reading the summaries and looking at the residual plots, I noticed a few things:\nFirstly, the linear model for the overall scores is generally a good fit. However, the recommendation score is not a great fit. There are four plots that we look at for determining if a linear model is a good fit. Those plots are Residuals vs. Fitted, Q-Q residuals, Scale-Location, and Residuals vs. Leverage. To make things short, we want certain shapes for each of these graphs to determine if the model is a good fit. For the Residuals vs. Fitted, Scale-Location, and Residuals vs. Leverage graphs, we want them generally to be scattered about a horizontal line. The Q-Q residuals is different in the sense that we want a diagonal line. For the Residuals vs. Leverage graph, we want our data to form a sort of cone shape that is limited by Cook’s distance. Data that is outside that distance tends to skew the model. Notice how neither of the models really go past that line.\nSecondly, there are a few data points that stick out and are labeled on the graphs. I did go back through the data to observe those, and they did have higher scores than anticipated, but I chalked it up to great experiences at the hospital.\nThirdly, we look at the r-squared values for each of the models in their summaries. Typically, the larger the value, the more precise the predictor variables are able to predict the overall or recommended score. With this knowledge, the overall linear model is a better model than the recommended model (0.8596 vs. 0.7812).\nLastly, I looked through the values for the predictive equation. The equations are as follows:\n\noverall = -17.42 + 0.38(nurse_com) + 0.13(doc_com) + 0.03(staff_response) - 0.04(med_coms) + 0.04 (discharge_info) + 0.51(care_transition) + 0.10(clean) + 0.07(quiet)\nrec = -49.96 + 0.51(nurse_com) + 0.16(doc_com) - 0.05 (staff_response) - 0.12(med_coms) + 0.03 (discharge_info) + 0.96(care_transition) + 0.08(clean) + 0.03(quiet)\n\nI was curious about what variable impacts the scores the most. With these equations, there is a heavy influence from the care transition for both models.\n\n\n\nDesign and Data Collection\n\n\nInsights\nThoughtfully proposes an inferential model, explains in general how the inference works, and interprets the results appropriately\n\n\nLimitations:\nIf I were to receive this data to help me choose a hospital to go to, I would rely more on the overall scores to help me make my decision over the recommended score. However, this data should not be the only research done for these hospitals. These models do not always account for every different experience that a person would get, and some may be skewed positive or skewed negative experiences. The response rates for each of the hospitals varies incredibly, which can also be a factor in what data they receive. In looking at the data collection process, it seems that most of the surveys were of a voluntary response, indicating that mostly those who are passionate about their experience will respond. This may not be truly random data and should be taken with a grain of salt. If used in the real world, the model would affect those who were trying to make a decision on what hospital they should use. If they tried to choose the best hospital and got one that had more skewed results, they might not get the standard of care that they anticipated which could lead to a worse experience. It could also very much rule out great hospitals that got bad reviews from some unfortunate experiences. With this, certain hospitals could get over-crowded if there are a lot of good reviews rating them higher. This in turn could cause for more bad reviews as people wait longer for their care. Overall, this model could have a lot of influence on how these hospitals perform and how the people going to these hospitals could receive their care."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]